{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all parquet files\n",
    "# !tar chvfz customer_big.tar.gz bigdata2021data/customer_big.parquet/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_download_link(df, csv_file_name, delete_prompt=False):\n",
    "    \"\"\"Display a download link to load a data frame as csv from within a Jupyter notebook\"\"\"\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    from IPython.display import FileLink\n",
    "    display(FileLink(csv_file_name))\n",
    "    if delete_prompt:\n",
    "        a = input('Press enter to delete the file after you have downloaded it.')\n",
    "        import os\n",
    "        os.remove(csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_explore(df):\n",
    "    \"\"\"prints summary information of df\"\"\" \n",
    "    print('No. of rows: ', len(df))\n",
    "    print('*'*30)\n",
    "    \n",
    "    for col in df: # print summary of each variables\n",
    "        print(col)\n",
    "        print(df[col].unique()[:5])\n",
    "        print('Column index: ', df.columns.get_loc(col))\n",
    "        print('No. of unique entries: ',len(df[col].unique()))\n",
    "        print('Datatype: ', df[col].dtypes)\n",
    "        print('*'*30)\n",
    "    \n",
    "def col_type(df):\n",
    "    \"\"\"returns dictionary of column names that have a given datatype\"\"\"\n",
    "    g = df.columns.to_series().groupby(df.dtypes)\n",
    "    return {k.name: v for k, v in g.groups.items()}\n",
    "\n",
    "def convert2lower(df):\n",
    "    \"\"\"convert all string to lowercase and strips leading/trailing white space\"\"\" \n",
    "    for col in col_type(df)['object']:\n",
    "        df[col] = df[col].astype(str).str.lower()\n",
    "        df[col] = df[col].str.strip()\n",
    "    return df\n",
    "\n",
    "def zero_fill_nan(df, cols):\n",
    "    \"\"\"Replace NaN with zero\"\"\"\n",
    "    df[cols]=df[cols].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you are working on your own computer instead of the JupyterHub, you might have to install pyarrow to be able to open the following parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows:  4469\n",
      "******************************\n",
      "customer_id_mskd\n",
      "['df39588796bc20fb01d282213200c944ad38caa06a91a77c821d6396fc856ea5'\n",
      " 'd56f5632aa40d2661dcc5419def6ede2ed4cca24bd76cdb4ece0d6981e03ccb4'\n",
      " 'a86efc98b6e70bbb37c12024304f3d7c9eac9e4ecd9559fa02a6e29da5954344'\n",
      " '24f154d9bb05a18ba723830b6c41d6eb71a67f14ca73251876eb96de21d41260'\n",
      " 'bdb58c996ed0b5c9f81461caa8efe914f712e22ffaa6ac38157e8940a821bcb3']\n",
      "Column index:  0\n",
      "No. of unique entries:  4469\n",
      "Datatype:  object\n",
      "******************************\n",
      "jurisdiction_code\n",
      "['ca03' 'ca08' 'ca16']\n",
      "Column index:  1\n",
      "No. of unique entries:  3\n",
      "Datatype:  object\n",
      "******************************\n",
      "client_type_aml\n",
      "['individual']\n",
      "Column index:  2\n",
      "No. of unique entries:  1\n",
      "Datatype:  object\n",
      "******************************\n",
      "industry_code_aml\n",
      "['none']\n",
      "Column index:  3\n",
      "No. of unique entries:  1\n",
      "Datatype:  object\n",
      "******************************\n",
      "occupation_code_aml\n",
      "['108' '342' 'e902' '94' '256']\n",
      "Column index:  4\n",
      "No. of unique entries:  235\n",
      "Datatype:  object\n",
      "******************************\n",
      "country_of_domicile_aml\n",
      "['ca' 'be' 'ir' 'cn' 've']\n",
      "Column index:  5\n",
      "No. of unique entries:  66\n",
      "Datatype:  object\n",
      "******************************\n",
      "occupation_status_code_aml\n",
      "['none' 'e902' 'e904' 'e901' 'e900']\n",
      "Column index:  6\n",
      "No. of unique entries:  5\n",
      "Datatype:  object\n",
      "******************************\n",
      "customer_status_aml\n",
      "['active' 'not a customer' 'inactive']\n",
      "Column index:  7\n",
      "No. of unique entries:  3\n",
      "Datatype:  object\n",
      "******************************\n",
      "export_ts\n",
      "['2020-05-13T00:00:00.000000000']\n",
      "Column index:  8\n",
      "No. of unique entries:  1\n",
      "Datatype:  datetime64[ns]\n",
      "******************************\n",
      "primary_ownership_flag\n",
      "['n' 'y' 'none']\n",
      "Column index:  9\n",
      "No. of unique entries:  3\n",
      "Datatype:  object\n",
      "******************************\n",
      "relationship_type\n",
      "['joint' 'borrower' 'primary joint' 'none' 'co-borrower']\n",
      "Column index:  10\n",
      "No. of unique entries:  15\n",
      "Datatype:  object\n",
      "******************************\n",
      "PCD_CDA\n",
      "[ 1. nan  2.  3.  4.]\n",
      "Column index:  11\n",
      "No. of unique entries:  10\n",
      "Datatype:  float64\n",
      "******************************\n",
      "PCD_CMS\n",
      "[nan]\n",
      "Column index:  12\n",
      "No. of unique entries:  1\n",
      "Datatype:  float64\n",
      "******************************\n",
      "PCD_CRC\n",
      "[nan  2.  3.  1.  5.]\n",
      "Column index:  13\n",
      "No. of unique entries:  10\n",
      "Datatype:  float64\n",
      "******************************\n",
      "PCD_LLC\n",
      "[nan  1.  3.  2.  5.]\n",
      "Column index:  14\n",
      "No. of unique entries:  8\n",
      "Datatype:  float64\n",
      "******************************\n",
      "PCD_MOR\n",
      "[nan  1.  2.  4.  3.]\n",
      "Column index:  15\n",
      "No. of unique entries:  7\n",
      "Datatype:  float64\n",
      "******************************\n",
      "PCD_MUF\n",
      "[nan]\n",
      "Column index:  16\n",
      "No. of unique entries:  1\n",
      "Datatype:  float64\n",
      "******************************\n",
      "PCD_SAV\n",
      "[nan  1.  2.  4.  3.]\n",
      "Column index:  17\n",
      "No. of unique entries:  12\n",
      "Datatype:  float64\n",
      "******************************\n",
      "PCD_SDB\n",
      "[nan  1.  2.]\n",
      "Column index:  18\n",
      "No. of unique entries:  3\n",
      "Datatype:  float64\n",
      "******************************\n",
      "PCD_TED\n",
      "[nan  1.  2.  3.  4.]\n",
      "Column index:  19\n",
      "No. of unique entries:  8\n",
      "Datatype:  float64\n",
      "******************************\n",
      "SRV_FLG\n",
      "[nan]\n",
      "Column index:  20\n",
      "No. of unique entries:  1\n",
      "Datatype:  float64\n",
      "******************************\n",
      "SRV_FSL\n",
      "[nan]\n",
      "Column index:  21\n",
      "No. of unique entries:  1\n",
      "Datatype:  float64\n",
      "******************************\n",
      "SRV_ILC\n",
      "[nan]\n",
      "Column index:  22\n",
      "No. of unique entries:  1\n",
      "Datatype:  float64\n",
      "******************************\n",
      "SRV_LOC\n",
      "[nan]\n",
      "Column index:  23\n",
      "No. of unique entries:  1\n",
      "Datatype:  float64\n",
      "******************************\n",
      "SRV_NLG\n",
      "[nan]\n",
      "Column index:  24\n",
      "No. of unique entries:  1\n",
      "Datatype:  float64\n",
      "******************************\n",
      "SRV_NSL\n",
      "[nan]\n",
      "Column index:  25\n",
      "No. of unique entries:  1\n",
      "Datatype:  float64\n",
      "******************************\n",
      "SRV_TRF\n",
      "[nan]\n",
      "Column index:  26\n",
      "No. of unique entries:  1\n",
      "Datatype:  float64\n",
      "******************************\n",
      "PRD_INFO_AVAIL\n",
      "[False  True]\n",
      "Column index:  27\n",
      "No. of unique entries:  2\n",
      "Datatype:  bool\n",
      "******************************\n",
      "rating\n",
      "[3. 1. 2.]\n",
      "Column index:  28\n",
      "No. of unique entries:  3\n",
      "Datatype:  float64\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# load the customer training data set:\n",
    "df1 = pd.read_parquet(\"cust_train.parquet\")\n",
    "# Preprocess df1\n",
    "df1 = convert2lower(df1)\n",
    "# df1.info()\n",
    "# df1.nunique()\n",
    "df_explore(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256     1054\n",
       "e902     756\n",
       "e904     616\n",
       "e901     418\n",
       "e900     274\n",
       "        ... \n",
       "15         1\n",
       "75         1\n",
       "230        1\n",
       "192        1\n",
       "244        1\n",
       "Name: occupation_code_aml, Length: 235, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.occupation_code_aml.unique()\n",
    "df1.occupation_code_aml.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows:  69534\n",
      "******************************\n",
      "customer_id_mskd\n",
      "['4eb76d305d32c1d00cc0d8850abe45ffc49f51f08324ee2c3070e0d59232776b'\n",
      " '510ad85cdbe68ad98edaf81cbc2fda6fc218d1c4acc53b974df2e85e2cc0d559'\n",
      " '8115b4ba5e267184283a64117e9664deb55159c1bd4e2ddfa5ae89a5aa531168'\n",
      " '6c854026065a868b6134d04d906dcfa9af7278ff834b0cbfcee19d9fc248d899'\n",
      " 'd8cc95339794bf3f84f941b1eba6266d58af1dc7e651782a44c83a03129cb16a']\n",
      "Column index:  0\n",
      "No. of unique entries:  2827\n",
      "Datatype:  object\n",
      "******************************\n",
      "month\n",
      "['2020-02' '2019-04' '2019-05' '2019-06' '2020-01']\n",
      "Column index:  1\n",
      "No. of unique entries:  13\n",
      "Datatype:  object\n",
      "******************************\n",
      "in_amt\n",
      "[ 150.  400.    0. 1200.   20.]\n",
      "Column index:  2\n",
      "No. of unique entries:  22485\n",
      "Datatype:  float64\n",
      "******************************\n",
      "in_cnt\n",
      "[1. 0. 2. 4. 3.]\n",
      "Column index:  3\n",
      "No. of unique entries:  58\n",
      "Datatype:  float64\n",
      "******************************\n",
      "out_amt\n",
      "[1600. 1000.  500.  315.    0.]\n",
      "Column index:  4\n",
      "No. of unique entries:  43436\n",
      "Datatype:  float64\n",
      "******************************\n",
      "out_cnt\n",
      "[2. 1. 6. 0. 4.]\n",
      "Column index:  5\n",
      "No. of unique entries:  247\n",
      "Datatype:  float64\n",
      "******************************\n",
      "trsactn_type\n",
      "['cash' 'cheque' 'amex' 'visa' 'debit']\n",
      "Column index:  6\n",
      "No. of unique entries:  5\n",
      "Datatype:  object\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# load the transaction training data set:\n",
    "df2 = pd.read_parquet(\"transaction_train.parquet\")\n",
    "df2 = zero_fill_nan(df2, ['in_amt', 'in_cnt', 'out_amt', 'out_cnt'])\n",
    "\n",
    "# Analyze data in df2\n",
    "df_explore(df2)\n",
    "# '510ad85cdbe68ad98edaf81cbc2fda6fc218d1c4acc53b974df2e85e2cc0d559' in df1['customer_id_mskd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_amt</th>\n",
       "      <th>in_cnt</th>\n",
       "      <th>out_amt</th>\n",
       "      <th>out_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.953400e+04</td>\n",
       "      <td>69534.000000</td>\n",
       "      <td>6.953400e+04</td>\n",
       "      <td>69534.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.273571e+03</td>\n",
       "      <td>1.489516</td>\n",
       "      <td>3.492482e+03</td>\n",
       "      <td>13.64984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.326930e+04</td>\n",
       "      <td>3.121156</td>\n",
       "      <td>4.316777e+04</td>\n",
       "      <td>24.28942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.150000e+02</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.800000e+02</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.075380e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.888535e+03</td>\n",
       "      <td>15.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.333333e+06</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>5.134000e+06</td>\n",
       "      <td>748.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             in_amt        in_cnt       out_amt      out_cnt\n",
       "count  6.953400e+04  69534.000000  6.953400e+04  69534.00000\n",
       "mean   3.273571e+03      1.489516  3.492482e+03     13.64984\n",
       "std    6.326930e+04      3.121156  4.316777e+04     24.28942\n",
       "min    0.000000e+00      0.000000  0.000000e+00      0.00000\n",
       "25%    0.000000e+00      0.000000  1.150000e+02      1.00000\n",
       "50%    5.000000e+01      1.000000  5.800000e+02      4.00000\n",
       "75%    1.075380e+03      2.000000  1.888535e+03     15.00000\n",
       "max    8.333333e+06    172.000000  5.134000e+06    748.00000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2020-02', '2019-04', '2019-05', '2019-06', '2020-01', '2019-11',\n",
       "       '2019-08', '2019-09', '2019-07', '2019-12', '2020-03', '2019-10',\n",
       "       '2020-04'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.month.unique()\n",
    "#out_count = df2.groupby('out_amt').count()\n",
    "#out_count.sort_values(by=['out_amt'], ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'customer_big.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0d3bef04d846>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load the large customer data set:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"customer_big.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# this might fail because of memory constraints!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     return impl.read(\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m     )\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"filesystem\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m         )\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# fsspec resources can also point to directories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mhandles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mpath_or_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'customer_big.parquet'"
     ]
    }
   ],
   "source": [
    "# load the large customer data set:\n",
    "df3 = pd.read_parquet(\"bigdata2021data/customer_big.parquet\")\n",
    "\n",
    "# this might fail because of memory constraints!\n",
    "\n",
    "df3.groupby('client_type_aml').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# however, you can still load individual parts of that large file as it is partitioned into 200 individual files\n",
    "df3_0 = pd.read_parquet(\"bigdata2021data/customer_big.parquet/part-00000-9799628b-5c6c-499c-a655-f339d13ed4c0-c000.snappy.parquet\")\n",
    "df3_0 = convert2lower(df3_0)\n",
    "# Analyze data in df3_0\n",
    "df_explore(df3_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_0['relationship_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the large transaction data set:\n",
    "df4 = pd.read_parquet(\"bigdata2021data/transaction_big.parquet\")\n",
    "df4\n",
    "# this might fail because of memory constraints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# however, you can still load individual parts of that large file as it is partitioned into 122 individual files\n",
    "df4_0 = pd.read_parquet(\"bigdata2021data/transaction_big.parquet/part-00000-d7f04269-6d76-4ab2-bd40-0229b3885a23-c000.snappy.parquet\")\n",
    "df4_0 = convert2lower(df4_0)\n",
    "\n",
    "df_explore(df4_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_1 = pd.read_parquet(\"bigdata2021data/transaction_big.parquet/part-00001-d7f04269-6d76-4ab2-bd40-0229b3885a23-c000.snappy.parquet\")\n",
    "df4_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tiny toy example for saving files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"filename.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Easy Case:</b> Automate an Expert using labelled data\n",
    "    <br>\n",
    "    - This makes use of labels provided by experts in the “customer_train” table\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised ML\n",
    "# @input: \n",
    "# @output: rating (multiclass logistic)\n",
    "\n",
    "# Algorithm: SGD Classifier \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def OneHotEncode_col(df, col_index):\n",
    "#     col_index = df.columns.get_loc(col_name)\n",
    "#     @col_index : array\n",
    "    ind_pass = np.delete(np.arange(0, len(df.columns)), col_index)\n",
    "    \n",
    "    c_transform = ColumnTransformer([('onehot', OneHotEncoder(), col_index), ('nothing', 'passthrough', ind_pass)])\n",
    "    x = df.values\n",
    "    return c_transform.fit_transform(x).astype(float).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remove_easy = [0, 2, 3, 8, 12, 16, *range(20,26+1)]\n",
    "df_easy = df1.drop(df1.columns[col_remove_easy], axis=1)\n",
    "df_easy.fillna(0, inplace=True) # fill nan with 0\n",
    "X_easy = OneHotEncode_col(df_easy.iloc[:, :-1], [*range(0, 6+1), 14])\n",
    "y_easy = df_easy['rating'].values # rating column\n",
    "\n",
    "# shuffle\n",
    "idx = np.arange(X_easy.shape[0])\n",
    "np.random.seed(9)\n",
    "np.random.shuffle(idx)\n",
    "X_easy = X_easy[idx]\n",
    "y_easy = y_easy[idx]\n",
    "\n",
    "\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_easy, y_easy, test_size=0.33)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SGDClassifier(alpha=0.001, max_iter=100)).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 2., 3., ..., 2., 1., 1.])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(X_test)\n",
    "clf.predict(X_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct? : 1367 out of 1475 : 92.7%\n",
      "True Positive (3) : 405 out of 1475 : 27.5%\n",
      "False Positive (3) : 8 out of 1475 : 0.5%\n",
      "True Negative (3) : 962 out of 1475 : 65.2%\n",
      "True-ish Negative (3) : 58 out of 1475 : 3.9%\n",
      "False Negative (3) : 42 out of 1475 : 2.8%\n"
     ]
    }
   ],
   "source": [
    "compare = pd.DataFrame({'rating_obs':y_test, 'rating_pred':clf.predict(X_test)}, columns=['rating_obs','rating_pred'])\n",
    "rating_risk = 3\n",
    "\n",
    "compare['Correct?'] = np.where(\n",
    "    compare['rating_obs'] == compare['rating_pred'], 1, 0)\n",
    "compare['True Positive (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] == rating_risk) & (compare['rating_pred'] == rating_risk), 1, 0)\n",
    "compare['False Positive (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] != rating_risk) & (compare['rating_pred'] == rating_risk), 1, 0)\n",
    "compare['True Negative (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] != rating_risk) & (compare['rating_pred'] != rating_risk) & (compare['Correct?'] == 1), 1, 0)\n",
    "compare['True-ish Negative (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] != rating_risk) & (compare['rating_pred'] != rating_risk) & (compare['Correct?'] == 0), 1, 0)\n",
    "compare['False Negative (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] == rating_risk) & (compare['rating_pred'] != rating_risk), 1, 0)\n",
    "\n",
    "for i in range(1, 7):\n",
    "    col = compare.columns[i+1]\n",
    "    percent = '{:.1%}'.format(compare[col].sum()/len(compare))\n",
    "    print(f'{col} : {compare[col].sum()} out of {len(compare)} : {percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Medium Case:</b> Improve the accuracy by augmenting the data\n",
    "    <br>\n",
    "    - A smaller “transaction_train” table is an easy place to start augmenting. This does not correspond one-to-one with the customer table, so you will have to figure out some way to combine them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised ML\n",
    "# @input: \n",
    "# @output: rating (multiclass logistic)\n",
    "\n",
    "# sort by month to keep temporal series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remove_med = [2, 3, 8, 12, 16, *range(20,26+1)]\n",
    "\n",
    "df_med = df1.drop(df1.columns[col_remove_med], axis=1)\n",
    "df_med.fillna(0, inplace=True) # fill nan with 0\n",
    "df_med = pd.merge(df_med, df2, on='customer_id_mskd', how='outer')\n",
    "df_med = df_med[df_med['month'].notna()] # Remove nan from month\n",
    "df_med['month'] = pd.to_datetime(df_med.month)\n",
    "df_med.sort_values(by='month', inplace=True)\n",
    "\n",
    "dict_date = dict(enumerate(df_med['month'].unique()))\n",
    "dict_date = dict((v,k) for k,v in dict_date.items())\n",
    "\n",
    "df_med.replace({\"month\": dict_date}, inplace=True)\n",
    "\n",
    "X_med = OneHotEncode_col(df_med, [*range(0, 7+1), 15, 22])\n",
    "y_med = df_med['rating'].values # rating column\n",
    "\n",
    "# shuffle\n",
    "idx = np.arange(X_easy.shape[0])\n",
    "np.random.seed(7)\n",
    "np.random.shuffle(idx)\n",
    "X_med = X_med[idx]\n",
    "y_med = y_med[idx]\n",
    "\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_med, y_med, test_size=0.33)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SGDClassifier(alpha=0.001, max_iter=100)).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct? : 92.7%\n",
      "True Positive (3) : 27.5%\n",
      "False Positive (3) : 0.5%\n",
      "True Negative (3) : 65.2%\n",
      "True-ish Negative (3) : 3.9%\n",
      "False Negative (3) : 2.8%\n"
     ]
    }
   ],
   "source": [
    "compare = pd.DataFrame({'rating_obs':y_test, 'rating_pred':clf.predict(X_test)}, columns=['rating_obs','rating_pred'])\n",
    "rating_risk = 3\n",
    "\n",
    "compare['Correct?'] = np.where(\n",
    "    compare['rating_obs'] == compare['rating_pred'], 1, 0)\n",
    "compare['True Positive (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] == rating_risk) & (compare['rating_pred'] == rating_risk), 1, 0)\n",
    "compare['False Positive (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] != rating_risk) & (compare['rating_pred'] == rating_risk), 1, 0)\n",
    "compare['True Negative (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] != rating_risk) & (compare['rating_pred'] != rating_risk) & (compare['Correct?'] == 1), 1, 0)\n",
    "compare['True-ish Negative (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] != rating_risk) & (compare['rating_pred'] != rating_risk) & (compare['Correct?'] == 0), 1, 0)\n",
    "compare['False Negative (' + str(rating_risk)+ ')'] = np.where(\n",
    "    (compare['rating_obs'] == rating_risk) & (compare['rating_pred'] != rating_risk), 1, 0)\n",
    "\n",
    "for i in range(1, 7):\n",
    "    col = compare.columns[i+1]\n",
    "    percent = '{:.1%}'.format(compare[col].sum()/len(compare))\n",
    "    print(f'{col} : {compare[col].sum()} out of {len(compare)} : {percent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hard Case:</b> Identify accounts without relying on experts\n",
    "    <br>\n",
    "    - This is unsupervised learning, and can most likely benefit from as much data as you can manage. There is an awful lot of unlabelled data compared to the labelled stuff.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersupervised -> Supervised ML\n",
    "# @input: \n",
    "# @output: clusters (filtered)\n",
    "\n",
    "# Run supervised ML on each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
